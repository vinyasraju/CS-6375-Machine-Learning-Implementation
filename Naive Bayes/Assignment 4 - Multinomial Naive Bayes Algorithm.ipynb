{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import math\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For prototyping MNB\n",
    "# trainInstances = [[\"Chinese\", \"Beijing\", \"Chinese\"], \n",
    "#             [\"Chinese\", \"Chinese\", \"Shanghai\"],\n",
    "#             [\"Chinese\", \"Macao\"], \n",
    "#             [\"Tokyo\", \"Japan\", \"Chinese\"]]\n",
    "# trainClassLabels = [\"YES\", \"YES\", \"YES\", \"NO\"]\n",
    "\n",
    "# testInstances = [[\"Chinese\", \"Chinese\", \"Chinese\", \"Tokyo\", \"Japan\"]]\n",
    "# actualTestLabels = [\"NO\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#creates vector of training and testing instances\n",
    "def createData(selected5Classes, path):\n",
    "    instances = []\n",
    "    classLabels = []\n",
    "    \n",
    "    for file in selected5Classes:\n",
    "        filename = file\n",
    "        allfiles = os.listdir(path+\"\\\\\"+filename)\n",
    "        \n",
    "        for mainfile in allfiles:\n",
    "            size = len(allfiles)\n",
    "            currentFilePath = path+\"\\\\\"+filename+\"\\\\\"+mainfile\n",
    "            vector = getVectorOfFile(currentFilePath)\n",
    "            instances.append(vector)\n",
    "            classLabels.append(filename)\n",
    "        \n",
    "    return instances, classLabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#remove special characters from data\n",
    "def removeNonAplhabet(vector):\n",
    "    return [w.lower() for w in vector if w.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#get the vocabulary of training instances\n",
    "def extractVocabulary(trainInstances):\n",
    "    totalTrainInstances = len(trainInstances)\n",
    "    vocabulary = []\n",
    "    for i in range(totalTrainInstances):\n",
    "        vocabulary = vocabulary + trainInstances[i]\n",
    "    vocabulary = list(set(vocabulary))\n",
    "    return vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#removes header from file and converts text to tokens\n",
    "def getVectorOfFile(currentFilePath):\n",
    "    flag = 0\n",
    "    tokens = []\n",
    "\n",
    "    currentFile = open(currentFilePath, 'r')\n",
    "\n",
    "    for line in currentFile:\n",
    "        if line.find(\"Lines\") == -1:\n",
    "            if(flag != 0):\n",
    "                word_tokens = word_tokenize(line)\n",
    "                filteredLine = [w for w in word_tokens if not w in stop_words]\n",
    "                tokens.extend(filteredLine)\n",
    "        else:\n",
    "            flag = 1\n",
    "            \n",
    "    currentFile.close()        \n",
    "    tokens = removeNonAplhabet(tokens)\n",
    "    tokens.sort()\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#calculates the accuracy of MNB\n",
    "def getAccuracy(testInstances, actualTestLabels, C, V, prior, conditionalProb):\n",
    "    correctCount = 0\n",
    "    size = len(testInstances)\n",
    "    \n",
    "    for i in range(size):\n",
    "        percentage = (i+1)*100/size\n",
    "        d = testInstances[i]\n",
    "        prediction = applyMultinomialNB(C, V, prior, conditionalProb, d)\n",
    "        if(prediction == actualTestLabels[i]):\n",
    "            correctCount = correctCount + 1\n",
    "            \n",
    "    return (correctCount/size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#count total docs in training set\n",
    "def countDocs(trainInstances):\n",
    "    return len(trainInstances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#get the unique class labels\n",
    "def getClassLabels(classLabels):\n",
    "    return list(set(classLabels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#count total number of documents in a given class c\n",
    "def countDocsInClass(classLabels, c):\n",
    "    return classLabels.count(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#make a vector of all tokens in all instances of class c\n",
    "def concatenateTextOfAllDocsInClass(trainInstances, classLabels, c):\n",
    "    text_c = []\n",
    "    for i in range(len(trainInstances)):\n",
    "        if(classLabels[i]==c):\n",
    "            for j in range(len(trainInstances[i])):\n",
    "                text_c.append(trainInstances[i][j])\n",
    "            \n",
    "    return text_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#counts occurences\n",
    "def countTokensOfTerm(text_c, t):\n",
    "    return text_c.count(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#gets all tokens in class classLabel\n",
    "def getTotalFeaturesInClass(classLabel, trainInstances, classLabels):\n",
    "    count = 0\n",
    "    for i in range(len(trainInstances)):\n",
    "        if(classLabels[i] == classLabel):\n",
    "            count = count + len(trainInstances[i])\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#for testing, test on only those which words exist in the vocabulary\n",
    "def extractTokensFromDoc(V, d):\n",
    "    W = []\n",
    "    for i in range(len(d)):\n",
    "        t = d[i]\n",
    "        if(t in V):\n",
    "            W.append(t)\n",
    "    return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#returns index of token t in vocabulary\n",
    "def getIndexOfTermInV(t, V):\n",
    "    return V.index(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#algorithm to train classifier\n",
    "def trainMultinomialNB(C, D, classLabels):\n",
    "    V = extractVocabulary(D)\n",
    "    N = countDocs(D)\n",
    "    prior = []\n",
    "    conditionalProb = np.zeros((len(V), len(C)))\n",
    "    B = len(V)\n",
    "    \n",
    "    for index_of_c in range(len(C)):\n",
    "        c = C[index_of_c]\n",
    "        Nc = countDocsInClass(classLabels, c)\n",
    "        prior.append(Nc/N)\n",
    "        text_c = concatenateTextOfAllDocsInClass(D, classLabels, c)\n",
    "        Sum_Tct_prime = getTotalFeaturesInClass(c, D, classLabels)\n",
    "        size = len(V)\n",
    "        \n",
    "        for index_of_t in range(size): \n",
    "            t = V[index_of_t]\n",
    "            Tct = countTokensOfTerm(text_c, t)\n",
    "            conditionalProb[index_of_t][index_of_c] =  (Tct+1)/(Sum_Tct_prime+B)\n",
    "    \n",
    "    return V, prior, conditionalProb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#argmax computation, maximize the probablity of class c given instance x\n",
    "def applyMultinomialNB(C, V, prior, conditionalProb, d):\n",
    "    W = extractTokensFromDoc(V, d)\n",
    "    #print(W)\n",
    "    score = []\n",
    "    for index_of_c in range(len(C)):\n",
    "        c = C[index_of_c]\n",
    "        #print(\"Current Class:\", c)\n",
    "        score.append(math.log(prior[index_of_c]))\n",
    "    \n",
    "        for index_of_t in range(len(W)):\n",
    "            t = W[index_of_t]\n",
    "            index_of_t_in_V = getIndexOfTermInV(t, V)\n",
    "            score[index_of_c] = score[index_of_c] + math.log(conditionalProb[index_of_t_in_V][index_of_c])\n",
    "        #print(c,score[index_of_c])\n",
    "    \n",
    "    index_prediction = np.argmax(score)\n",
    "    prediction = C[index_prediction]\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the path to the Training Dataset: 20news-bydate\\\\20news-bydate-test\n",
      "Enter the path to the Testing Dataset: 20news-bydate\\\\20news-bydate-train\n"
     ]
    }
   ],
   "source": [
    "#input from user\n",
    "trainPath = input(\"Enter the path to the Training Dataset: \")\n",
    "testPath = input(\"Enter the path to the Testing Dataset: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainPath = \"20news-bydate\\\\20news-bydate-test\"\n",
    "# testPath = \"20news-bydate\\\\20news-bydate-train\"\n",
    "\n",
    "labels = os.listdir(trainPath)\n",
    "\n",
    "#select 5 classes randomly\n",
    "random.shuffle(labels)\n",
    "\n",
    "selected5Classes = labels[0:5]\n",
    "#print(\"The 5 selected classes:\", selected5Classes)\n",
    "\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#the creation of vectors, training and testing takes time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the tokens from files of various classes into vectors\n",
    "trainInstances, trainClassLabels = createData(selected5Classes, trainPath)\n",
    "testInstances, actualTestLabels = createData(selected5Classes, testPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training the classifier\n",
    "C = getClassLabels(trainClassLabels)\n",
    "D = trainInstances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "V, prior, conditionalProb = trainMultinomialNB(C, D, trainClassLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = getAccuracy(testInstances, actualTestLabels, C, V, prior, conditionalProb) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"The 5 selected classes:\\n\", selected5Classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy is: {:.3f}\".format(accuracy*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
