{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from math import exp\n",
    "from operator import mul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def removeNullFromData(dataframe):\n",
    "    updated_dataframe = dataframe\n",
    "    for i in range(len(dataframe)):\n",
    "        for j in range(len(dataframe.iloc[i, :])):\n",
    "            if(dataframe.iloc[i,j] == '?'):\n",
    "                updated_dataframe = updated_dataframe.drop(dataframe.index[i])\n",
    "                break\n",
    "\n",
    "    return updated_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def meanNormalize(array):\n",
    "    mean = np.average(array)\n",
    "    std_dev = np.std(array)\n",
    "    normalizedArray = (array - mean)/std_dev\n",
    "    return normalizedArray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def standardizeData(dataframe_without_nulls):\n",
    "    normalised_dataframe = dataframe_without_nulls\n",
    "    for i in range(len(dataframe_without_nulls.columns)):\n",
    "        data_type = dataframe_without_nulls.dtypes[i]\n",
    "        if(data_type == np.int64 or data_type == np.float64):\n",
    "            normalised_dataframe[i] = meanNormalize(dataframe_without_nulls[i])\n",
    "    return normalised_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def encodeLabels(normalised_dataframe):\n",
    "    preprocessed_dataframe = normalised_dataframe\n",
    "    for j in range(len(preprocessed_dataframe.columns)):\n",
    "        data_type = preprocessed_dataframe.dtypes[j]\n",
    "        if(data_type == np.object): \n",
    "            list_unique = preprocessed_dataframe[j].unique().tolist()\n",
    "            for i in range(len(preprocessed_dataframe)):\n",
    "                value = preprocessed_dataframe.iloc[i, j]\n",
    "                label = list_unique.index(value)\n",
    "                preprocessed_dataframe.iloc[i, j] = label\n",
    "    return preprocessed_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normaliseClass(preprocessed_dataframe):\n",
    "    j = len(preprocessed_dataframe.columns)-1\n",
    "    preprocessed_dataframe[j]\n",
    "    list_unique = preprocessed_dataframe[j].unique().tolist()\n",
    "\n",
    "    a = list(list_unique)\n",
    "    n = len(a)\n",
    "    interval = 1/(n-1)\n",
    "    b = a\n",
    "    value = 0\n",
    "    for i in range(len(a)):\n",
    "        b[i] = value\n",
    "        value = value + interval\n",
    "\n",
    "    b = np.array(b)\n",
    "\n",
    "\n",
    "    for i in range(len(preprocessed_dataframe)):\n",
    "        value = preprocessed_dataframe.iloc[i, j]\n",
    "        #print(value)\n",
    "        label = b[list_unique.index(value)]\n",
    "        preprocessed_dataframe.iloc[i, j] = label\n",
    "        \n",
    "    return preprocessed_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def writeToFile(processed_dataframe, output_path):\n",
    "    processed_dataframe.to_csv(output_path, sep=',',index= None, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the URL of the dataset:https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\n",
      "Enter the output path for processed data:irisdataset.csv\n"
     ]
    }
   ],
   "source": [
    "#url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\"\n",
    "#url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/car/car.data\"\n",
    "#url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\"\n",
    "#output_path = \"irisdataset.csv\"\n",
    "\n",
    "url = input(\"Enter the URL of the dataset:\")\n",
    "output_path = input(\"Enter the output path for processed data:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre Processing has been done\n"
     ]
    }
   ],
   "source": [
    "dataframe = pd.read_csv(url, skipinitialspace=True, na_values='.', header = None)\n",
    "dataframe_without_nulls = removeNullFromData(dataframe)\n",
    "normalised_dataframe = standardizeData(dataframe_without_nulls)\n",
    "preprocessed_dataframe = encodeLabels(normalised_dataframe)\n",
    "processed_dataframe = normaliseClass(preprocessed_dataframe)\n",
    "writeToFile(processed_dataframe, output_path)\n",
    "print(\"Pre Processing has been done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "writeToFile(processed_dataframe, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#processing has been done, now model evaluation and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#sigmoid activation function\n",
    "#the condition is set to prevent overflow/underflow\n",
    "def sigmoid(net):\n",
    "    if(net > 50):\n",
    "        net = 50\n",
    "    elif(net<-50):\n",
    "        net = -50\n",
    "    return 1.0 / (1.0 + exp(-net))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#function to train network\n",
    "def trainNetwork(max_iterations, dataframe, list_weights, count_nodes, learning_rate):\n",
    "    for n in range(max_iterations):    \n",
    "        for i in range(len(dataframe)):\n",
    "            input_vector = dataframe.iloc[i,:-1]\n",
    "            input_vector = list(input_vector)\n",
    "            input_vector.insert(0, 1)\n",
    "            input_vector = np.array(input_vector)        \n",
    "            target = dataframe.iloc[i,-1]\n",
    "            list_sigmoid, output = forward_propagation(list_weights, input_vector, count_nodes)\n",
    "            list_weights = back_propagation(list_weights, input_vector, list_sigmoid, count_nodes, learning_rate, target)\n",
    "    return list_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create a network with randomly initialised weights\n",
    "def createWeights(count_input_nodes, count_hidden_layers, count_hidden_nodes):\n",
    "    count_hidden_nodes.insert(0, count_input_nodes)\n",
    "    count_nodes = count_hidden_nodes\n",
    "    list_weights = list()\n",
    "    \n",
    "    for i in range(count_hidden_layers):\n",
    "        j = i + 1\n",
    "        W = np.matrix(np.random.uniform(-1,1, size=(count_nodes[j]-1, count_nodes[i])))\n",
    "        list_weights.append(W.tolist())\n",
    "\n",
    "    i = i + 1\n",
    "    j = i + 1\n",
    "    W = np.matrix(np.random.uniform(-1,1, size=(count_output_nodes, count_nodes[i])))\n",
    "    list_weights.append(W.tolist())\n",
    "    return list_weights, count_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#calculate and return the accuracy of the model\n",
    "def getAccuracy(dataframe, list_weights, count_nodes):\n",
    "    j = len(dataframe.columns)-1\n",
    "    dataframe[j]\n",
    "    list_unique = dataframe[j].unique().tolist()\n",
    "    list_unique.sort()\n",
    "    num_corrects = 0\n",
    "    for i in range(len(dataframe)):\n",
    "        input_vector = dataframe.iloc[i,:-1]\n",
    "        input_vector = list(input_vector)\n",
    "        input_vector.insert(0, 1)\n",
    "        input_vector = np.array(input_vector)\n",
    "        target = dataframe.iloc[i,-1]\n",
    "        list_sigmoid, output = forward_propagation(list_weights, input_vector, count_nodes)\n",
    "        label  = findLabel(list_unique, output[0])\n",
    "        if(label == target):\n",
    "            num_corrects = num_corrects+1\n",
    "    accuracy = num_corrects/len(dataframe)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#forward pass\n",
    "def forward_propagation(list_weights, input_vector, count_nodes):\n",
    "    list_sigmoid = list()\n",
    "    \n",
    "    for i in range(len(count_nodes)):\n",
    "        temp = 0\n",
    "        sigmoid_at_level = list()\n",
    "        sigmoid_at_level.append(1)\n",
    "        if(i==0):\n",
    "            vector = input_vector\n",
    "        else:\n",
    "            vector\n",
    "            \n",
    "        temp = np.array(vector)*list_weights[i]\n",
    "        net = list()\n",
    "        for j in range(len(temp)):\n",
    "            net.append(sum(temp[j]))\n",
    "        \n",
    "        for j in range(len(net)):\n",
    "                sigmoid_at_level.append(sigmoid(net[j]))\n",
    "            \n",
    "        list_sigmoid.append(sigmoid_at_level)\n",
    "        vector = sigmoid_at_level\n",
    "    \n",
    "    output = sigmoid_at_level[1:]\n",
    "    return list_sigmoid, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#to find closest label\n",
    "def findLabel(array,value):\n",
    "    index = np.searchsorted(array, value, side=\"left\")\n",
    "    if index > 0 and (index == len(array) or math.fabs(value - array[index-1]) < math.fabs(value - array[index])):\n",
    "        return array[index-1]\n",
    "    else:\n",
    "        return array[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#back propagation\n",
    "\n",
    "def back_propagation(list_weights, input_vector, list_sigmoid, count_nodes, learning_rate, target):\n",
    "    #print(\"Back Prop\")\n",
    "    output = list_sigmoid[-1][1]\n",
    "    x = len(count_nodes)\n",
    "    \n",
    "    list_delta = list()\n",
    "    \n",
    "    for i in range(len(count_nodes), 0, -1):\n",
    "        delta_at_level = list()\n",
    "        delta_weights_level = 0\n",
    "        if(i==len(count_nodes)):\n",
    "            temp = (target - output)*output*(1-output)\n",
    "            delta_at_level.insert(0, temp)\n",
    "            list_delta.insert(0, delta_at_level)\n",
    "        else:\n",
    "            delta_at_level = np.array(list_sigmoid[i-1]) * (1 - np.array(list_sigmoid[i-1])) * np.array(list_delta[x-i-1]) * np.array(list_weights[i])\n",
    "            delta_weights_level = learning_rate * list_delta[x-i-1][0] * np.array(list_sigmoid[i-1])\n",
    "            list_delta.insert(0, delta_at_level.tolist())\n",
    "            mytemp = list_weights[i][0] + delta_weights_level\n",
    "            list_weights[i][0] = list(mytemp)\n",
    "            \n",
    "    delta_weights_level = learning_rate * np.matrix(list_delta[0][0][1:]).T * np.matrix(input_vector)\n",
    "    delta_weights_level = delta_weights_level.tolist()\n",
    "    \n",
    "    a = np.matrix(delta_weights_level)\n",
    "    b = np.matrix(list_weights[0])\n",
    "    c = np.add(a,b)\n",
    "    temp = c.tolist()\n",
    "    list_weights[0] = temp\n",
    "    \n",
    "    return list_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printWeights(list_weights, num_layers):\n",
    "    for i in range(num_layers):\n",
    "        print(\"\\n\\nLayer \", i)\n",
    "        list_weights_layer = list_weights[i]\n",
    "    \n",
    "        for k in range(len(list_weights_layer[0])):\n",
    "            print(\"\\n\\tNeuron\", k, \"weights:\\n\")\n",
    "            for j in range(len(list_weights_layer)):\n",
    "                print(\"\\t\\t\",list_weights_layer[j][k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the input Dataset: irisdataset.csv\n",
      "Enter the training percent: 90\n",
      "Enter the maximum iteration:100\n",
      "Enter the number of hidden layer:2\n",
      "Enter the nodes in hidden layers: 3\n",
      "Enter the nodes in hidden layers: 2\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Following input is required from user\n",
    "\n",
    "path_input_dataset = input(\"Enter the input Dataset: \")\n",
    "dataframe = pd.read_csv(path_input_dataset, header=None)\n",
    "dataframe = dataframe.sample(frac=1)\n",
    "training_percent = int(input(\"Enter the training percent: \"))\n",
    "\n",
    "max_iterations = int(input(\"Enter the maximum iteration:\"))\n",
    "learning_rate = 0.9\n",
    "\n",
    "count_features = len(dataframe.columns)-1+1 # -1 for the class label, +1 for bias\n",
    "\n",
    "count_hidden_layers = int(input(\"Enter the number of hidden layer:\"))\n",
    "\n",
    "count_hidden_nodes = list()\n",
    "\n",
    "for i in range(0, count_hidden_layers):\n",
    "    value = int(input(\"Enter the nodes in hidden layers: \"))\n",
    "    count_hidden_nodes.append(value+1)\n",
    "    \n",
    "count_input_nodes = count_features\n",
    "count_weight_matrices = count_hidden_layers + 1\n",
    "count_output_nodes = 1\n",
    "\n",
    "num_examples = len(dataframe)\n",
    "index = int(training_percent * num_examples / 100)\n",
    "training_dataset = dataframe[0:index]\n",
    "testing_dataset = dataframe[index:]\n",
    "\n",
    "list_weights, count_nodes = createWeights(count_input_nodes, count_hidden_layers, count_hidden_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_weights = trainNetwork(max_iterations, training_dataset, list_weights, count_nodes, learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traing Accuracy: 96.29629629629629\n"
     ]
    }
   ],
   "source": [
    "training_accuracy = getAccuracy(training_dataset, list_weights, count_nodes)\n",
    "print(\"Traing Accuracy:\", training_accuracy*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy: 100.0\n"
     ]
    }
   ],
   "source": [
    "testing_accuracy = getAccuracy(testing_dataset, list_weights, count_nodes)\n",
    "print(\"Testing Accuracy:\", testing_accuracy*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Error: 3.703703703703709\n",
      "Testing Error: 0.0\n"
     ]
    }
   ],
   "source": [
    "training_error = 1 - training_accuracy\n",
    "testing_error = 1 - testing_accuracy\n",
    "print(\"Training Error:\", training_error*100)\n",
    "print(\"Testing Error:\", testing_error*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Layer  0\n",
      "\n",
      "\tNeuron 0 weights:\n",
      "\n",
      "\t\t 3.1588803733075266\n",
      "\t\t 9.110814241479833\n",
      "\t\t -11.277689850062707\n",
      "\n",
      "\tNeuron 1 weights:\n",
      "\n",
      "\t\t 0.9200470446700328\n",
      "\t\t 0.4238475328198365\n",
      "\t\t -0.49751288140276184\n",
      "\n",
      "\tNeuron 2 weights:\n",
      "\n",
      "\t\t -3.1348040382665765\n",
      "\t\t 0.6380460803731025\n",
      "\t\t -0.9547593062220362\n",
      "\n",
      "\tNeuron 3 weights:\n",
      "\n",
      "\t\t 3.458158532501455\n",
      "\t\t -4.504601220914862\n",
      "\t\t 6.03452774653998\n",
      "\n",
      "\tNeuron 4 weights:\n",
      "\n",
      "\t\t 2.702720435491074\n",
      "\t\t -6.834094124725486\n",
      "\t\t 8.2598349873923\n",
      "\n",
      "\n",
      "Layer  1\n",
      "\n",
      "\tNeuron 0 weights:\n",
      "\n",
      "\t\t -2.23332752841\n",
      "\t\t -0.7196060608698471\n",
      "\n",
      "\tNeuron 1 weights:\n",
      "\n",
      "\t\t 19.5732652673\n",
      "\t\t -0.3604749268570595\n",
      "\n",
      "\tNeuron 2 weights:\n",
      "\n",
      "\t\t -17.0882418018\n",
      "\t\t 0.18014886480342218\n",
      "\n",
      "\tNeuron 3 weights:\n",
      "\n",
      "\t\t 22.4587260501\n",
      "\t\t -0.737175921696491\n",
      "\n",
      "\n",
      "Layer  2\n",
      "\n",
      "\tNeuron 0 weights:\n",
      "\n",
      "\t\t -1.11266054113\n",
      "\n",
      "\tNeuron 1 weights:\n",
      "\n",
      "\t\t 4.96372930317\n",
      "\n",
      "\tNeuron 2 weights:\n",
      "\n",
      "\t\t -5.61844329616\n"
     ]
    }
   ],
   "source": [
    "num_layers = len(list_weights)\n",
    "printWeights(list_weights, num_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
